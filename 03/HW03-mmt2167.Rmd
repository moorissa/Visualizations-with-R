---
title: "Homework 03"
author: "Moorissa Tjokro (mmt2167)"
output:
  html_notebook: 
    out_height:10 
    fig_height:10
---
## 1. Mosaic Plots

Using the Mosaic plots, display Punishment dataset in the vcd package. Discuss patterns of association between independent and dependent variables. Discuss single factor patterns as well as two and three factor interactions.  
```{r, echo=TRUE}
data(Punishment,package="vcd")
summary(Punishment)
str(Punishment)
```
### Brief observation on association between variables:
To see the patterns between the independent and dependent variables, we can first check the relationship between each variable in the dataset using the scatterplot matrix as follows. The **dependent variable** is frequency (Freq), and the **independent variables** are attitude, memory, education, and age.
```{r, echo=TRUE}
pairs(xtabs(Freq~attitude + memory + education + age,Punishment))
```

Based on our quick observation above, the frequency tends to go higher with attitude factor indicating moderate and memory factor indicating no, implying more people had no memories of corporal punishment as a child. For the three-level variables, the frequency tends to decrease as education level increases from elementary to middle to high school. Frequency also tends to increase as age increases as well.   
   
### Single factor pattern:

```{r, echo=TRUE}
library(vcd)
library(extracat)
a <- within(Punishment,attitude<-factor(attitude,levels=levels(attitude)[c(2,1)]))

mosaic(Freq ~ attitude, data=a, direction = c("h"),zero_size=0,
       gp = gpar(fill=c("lightblue","gray")),
       spacing = spacing_highlighting(), 
       main="Attitude Factor in Punishment Dataset")
```
The dataset represents the corporal punishment data from a study of the Gallup Institute in Denmark in 1979 about the attitude of a random sample
of 1,456 persons towards corporal punishment of children. In terms of frequency, we see clearly in the plot above that there are more people with attitudes indicating moderate punishment of children than no punishment.   
  
### Two factor interactions:
```{r, echo=TRUE}
# Two interactions:
mosaicplot(xtabs(Freq ~ attitude + education, data=Punishment),
           main="Attitude and Education Factors in Punishment Dataset", legend=TRUE, shade=TRUE)
```
The two interactions mosaic plot above extends the observation from the single pattern. In this case, we added `education` variable, where we see that in both types of attitude, the majority of people have an elementary school as their highest level of education, followed by secondary school, then high school with the  least number of people. This confirms our hypothesis in the first observation using scatterplot matrix, that the frequency tends to decrease with the increase in education. In this case, both variables are inversely correlated. While the frequency represents sizes of the mosaicplot, the different colors represent levels of the residuals. Note that the residuals for the independence model were not large (as shown in the legend), yet the association between education and attitude is highly significant.  
   
### Three Factor Interactions:
```{r, echo=TRUE}
a <- within(Punishment,attitude<-factor(attitude,levels=levels(attitude)[c(2,1)]))
mosaic(Freq ~ age + memory + attitude, data=a, direction = c("v","v","h"),zero_size=0,
       gp = shading_hcl,
       spacing = spacing_highlighting(), 
       main="Age, Attitude, and Memory Factors in Punishment Dataset")
```
In the three factor interactions, we observe the relationship between `age`, `attitude`, and `memory` and how they relate to the `frequency` variable (in this case our dependent variable). Looking at the sizes between variable groups, there are more people across all age groups (15-24, 25-39, and 40+ years of age) indicating that they had *no* memories of corporal punishment as a child (memory="no") than those who said *yes* in the survey. In each of these subsets broken down by memory and age variables, those who indicated moderate punishment of children (attitude="moderate") also dominate compared to those indicating no punishment of children (attitude="no"). Similar to the previous plot, the pearson residuals above also visualize the outcome of the independence test in the Punishment dataset. Those with small residual magnitudes are colored in gray. The negative residuals are shown in pink while higher positive residuals in blue.  
  
     
## 2. Heatmaps

**Using the IBM dataset, plot heatmaps for data exploration.**
Include 3 heatmaps in your assignment and describe the patterns you found.
You may sample, subset and summarize as you wish. (The numbers of rows and
columns in your heatmaps will have no effect on your grade.) Explain choices
regarding the ordering of rows / columns and scaling. Choose a perceptually
uniform color scale. (Note: if you scale the data yourself, rather than setting a
scale parameter, be aware the the base scale() function returns a matrix. Either
convert the result to a vector or write your own rescaling function.)


```{r}
# read the data
data=read.csv("WA_Sales_Products_2012-14.csv")
str(data) #88,475 rows and 11 columns
```

### Pattern 1:

The following heatmap is plotted using `Gross.margin`, `Quantity`, and `Revenue` variables. These features are chosen from the data because they represent continuous variables (note that years are rather discrete). The number of rows are a subset of the data containing 50 values in a random order, since this amount is a good amount to fit the heatmap well, and random so each time I run, the plot would show different patterns. The columns in the plot are ordered alphabetically and the scaling parameter uses `column` so that the variables are standardized individually across each feature, rather than comparing it with all available features. The frequency is scaled using a color scale as shown in the legend. In this case, we use a perceptually uniform color called inferno, which was taken from the `viridis` R package. The perceptually uniform color was chosen so that values close to each other have similar-appearing colors and values far away from each other have more different-appearing colors, consistently across the range of values. Furthermore, there does not seem to be any particular correlations between these variables in the plot. At first, it might look like that those with the highest quantity have the highest revenue, but if we look at other datapoints, there is no correlation of increase in values between both variables. We can observe that the highest values across features seem to be values in revenue and quantity, and gross margin seems to have the most variance of values in this plot.
  
```{r}
library(viridis)
subset1 = c("Revenue","Quantity","Gross.margin")

lmat = rbind(c(0,3),c(2,1),c(0,4))
lwid = c(1.5,40)
lhei = c(1.5,40,10)
margins = c(5, 5)

colors = c(seq(0,3,length=10),seq(3,9,length=10),seq(9,10,length=10))
color.palette = inferno
my_palette <- color.palette

# 1. Selecting only the numerical variables
# 2. Because size cannot be NA nor exceed 65536, I subset in random orders.
library(gplots) 
heatmap.2(as.matrix(data[sample(88475,50),subset1]),col=my_palette, density.info="none", #breaks=colors,
          Colv=FALSE, dendrogram="row", symkey=FALSE, symm=FALSE, symbreaks=TRUE, scale="column",
          trace="none", cexCol=1,margins = c(7, 3),lmat = rbind(c(4,3),c(2,1)),lwid = c(1.5,4),lhei = c(1.5,4))

# Options of colors: Magma, Inferno, Plasma and Viridis.
```

### Pattern 2:

The following heatmap is plotted using `Gross.margin`, `Quantity`, and `Revenue` variables, with a subset of the first 50 rows of the data. The features are chosen because they represent continuous variables (note that years are rather discrete), and the number of rows are chosen to be 50 because it is a good amount of data I could fit to see the pattern on the heatmap quite well. Columns in the plot are ordered alphabetically and the variables are standardized individually with normal transformation to z-score as you can see in the top left scale. The pattern here is that gross margin tends to have higher values than other variables, which have more of the blocks with equally shaded values. 


```{r}
library(viridis)
colors = c(seq(0,3,length=10),seq(3,9,length=10),seq(9,10,length=10))
color.palette1 = viridis
my_palette <- color.palette1

subset2 = c("Quantity","Revenue","Gross.margin")
heatmap.2(as.matrix(data[reorder(subset2,sample(1:100)),subset2]), col=my_palette, colsep=c(1:5), rowsep=(1:65), 
           sepwidth=c(0.05,0.05), sepcolor="white", trace="none",
           Rowv=T,Colv=F, key=FALSE, scale="none", dendrogram="none", 
           lhei = c(0.05,5), margins=c(1,8))
```
For subset two, I played around with the ordering of the features, which now has `Quantity`, `Revenue`, and `Gross.margin` respectively for the heatmap shown above. The graph is quite similar to the plot above, but I changed the color scaling with viridis which is also a perceptually uniform color scheme that would correspond well with the continuous variables. I also purposely reordered the data points to an increasing order to contrast with the first plot (here I used reorder() function as shown above). Additionally, I used the scale parameter as 'none' instead of 'column', which actually compares the revenue value to the rest of values in other features, which in this case are quantity and gross margin. We see a pattern that out of the three variables used in this heatmap, the largest values exist in the Revenue variable. Both values in the quantity variable (left) and gross margin (right) get to be very dark because they are relative in scale to Revenue variable.

### Pattern 3:

```{r}
subset3<-data
subset3<-na.omit(subset3)
subset3["Quantity"]<-as.numeric(unlist(subset3["Quantity"]))
subset3$QuarterTemp<-substr(subset3$Quarter,1,2)
subset3$Quarter<-paste(subset3$Year,subset3$QuarterTemp)
new_subset3 <- subset3 %>% group_by(Product, Retailer.country) %>% summarize(Quantity = sum(Quantity), Revenue=sum(Revenue), GrossMargin=mean(Gross.margin))
new_subset32 <- subset3 %>% group_by(Retailer.type, Quarter) %>% summarize(Quantity = sum(Quantity), Revenue=sum(Revenue), GrossMargin=mean(Gross.margin))
new_subset33 <- subset3 %>% group_by(Product.type, Quarter) %>% summarize(Quantity = sum(Quantity), Revenue=sum(Revenue), GrossMargin=mean(Gross.margin))

scaling <- function(x){ (x - mean(x)) / sd(x) }
new_subset3<-new_subset3%>% mutate (Revenue=scaling(Revenue),Quantity=scaling(Quantity))
new_subset32<-new_subset32%>% mutate (Revenue=scaling(Revenue),Quantity=scaling(Quantity))
new_subset33<-new_subset33%>% mutate (Revenue=scaling(Revenue),Quantity=scaling(Quantity))

new_subset3$Retailer.country<- reorder(new_subset3$Retailer.country, new_subset3$Revenue)
ggplot(new_subset3) + geom_tile(aes(x= Retailer.country, y= Product, fill = Revenue)) + 
    scale_fill_viridis(guide = guide_legend(title="Normalized Revenue"))+
    theme(axis.text.x=element_text(size=10,angle=-30, hjust= .1)) + 
    xlab("Country") +              
    ylab("Product")+theme(plot.title=element_text(hjust=0.5))

```
For the third heatmap, I wanted to create a quite different one from the two previous plots. The above graph was plotted using ggplot2 and viridis color scheme, and the data was also preprocessed to plot the a heatmap of product revenues by retail countries. We see here that for all the different types of products in Denmark, the revenue is the lowest among those of other countries plotted above. This plot has been reordered in an accending order in terms of their product revenues that has been rescaled and normalized for a single product. In this case, the x-axis represents countries going from the lowest revenue to the highest. Note that we see a very interesting pattern here, that most products from the United States have the highest revenue, which is consistent that Denmark is the one on the most left and United States being positioned on the most right of the plot. The product here is reordered in an alphabetically descending order to make it easier for viewers to read and search for a particular product by their first letter.

## 3. Missing Values

**Use imputation techniques to visually explore patterns of missing data in the College Scorecard Data.**
Sample and/or subset as you wish. Include 3 graphs that show missing
patterns. Describe the patterns you see. Choose the missing data patterns of a
single variable (or single group of variables) to discuss in more detail, drawing
on insight gleaned from the documentation, in addition to the graphs. Discuss
possible causes for the missing data patterns, and potential biases which might
result from these patterns.

```{r}
# read the data
scorecard=read.csv("Most-Recent-Cohorts-Scorecard-Elements.csv",na.string="NULL")
typeof(scorecard)

# str(scorecard) #7,703 rows and 122 columns
# when something is unexpected, do summary to figure out counts of nulls!
# summary(scorecard)
```

#### Graph 1:
```{r}
library(reshape2)
library(ggplot2)
library(dplyr)

ggplot_missing <- function(x){
  
  x %>% 
    is.na %>%
    melt %>%
    ggplot(data = .,
           aes(x = Var2,
               y = Var1)) +
    geom_raster(aes(fill = value)) +
    scale_fill_grey(name = "",
                    labels = c("Present","Missing")) +
    theme_minimal() + 
    theme(axis.text.x  = element_text(angle=45, vjust=0.5)) + 
    labs(x = "Variables in Dataset",
         y = "Rows / observations")
}
ggplot_missing(scorecard[1:10,25:30])
```
There is a really interesting pattern on the first ten datapoints of the dataset. Grey blocks represent missing values while those with solid blocks have non-missing values. One pattern we see from the plot is that the first, sixth and tenth datapoint have all present values across the chosen features while observation 3, 7, and 8 have all missing values across features. There are six different features of SAT percentiles that I picked to see patterns between them. Although both the number of features and observations are too small to decide on the missing pattern of all dataset, this subset is quite interesting enough to see: that there seems to be roughly as many missing datapoints in the set as the non-missing values. For a single variable, such as SATWR25 which represents the 25th percentile of SAT writing, we see that there are more missing data here than the non-missing values. However, this doesn't mean that there would be more missing values when represented as overall dataset. One possible cause is that the scores of enrolled students who are categorized in this group were not reported in many institutions. A potential bias which might result from this pattern would be an assumption that there would be as many missing values in the overall dataset as there are non-missing values. They could also ptentially expect that there would be many missing reports on observations within the 25th percentile of SAT writing category.


### Graph #2:
```{r}
visna(scorecard[,10:40], sort="b")
```
Most of the records in the Scorecard dataset apparently have missing values. The above graph represents different variables between 10th to 40th features of the dataset, as these positions have many missing values with different patterns. Based on the above plot where blue entries represent missing values, there are only two variables with no missing values. The plot suggests that ACTWR25, the 25th percentie on ACT Writing test score, have very often missing values compared to other features. In this plot, the ACTWR75, the 75th percentile on the same test, comes next, which identifies a correlation of high missing values within the ACT Writing test scores. The SATWR25 (25th Percentile of the SAT Writing) and SATWR75 (75th Percentile of the SAT Writing) come next in the feature as variables with many missing values, although the frequency of missingness is still less than ACTWR25. Taking an example of the ACTMT25, which represents the 25th percentile on ACT Math test score, although it has many missing values, it still has more avaiable data compared to average of ACT Writing test scores. It also shows some correlation to the 75th percent with the same category, which suggests us that a group of variables can have different groupings of percentiles but still show the same pattern of missing values. A possible cause of missing data pattern in this category that's not as many as the ACT Writing is that majority of enrolled students take the ACT tests but did not report immediately on the writing score. THis makes sense because usually it takes time for the graders to send back students standardized essays, unlike Math scores that can result in real-time when students take a computer test. Due to much more data presented in the math score, a potential bias would be to assume that the Math test scores in ACT is more reliable than the Writing scores, although the missingness pattern has not been evaluated to the whole dataset.

### Graph 3:
```{r}
library(mice)
# md.pattern(scorecard) 
library(datasets)
library(lattice)
library(VIM)
coltoinclude = scorecard[,16:30]
scorecard_aggr = aggr(coltoinclude, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(coltoinclude), cex.axis=.7, gap=3, ylab=c("Proportion for Missing Values","Pattern for Missing Values"))
```
In this subset of datapoints, I use `mice` package to help me visualize the missing values in a creative way. This way, I can add barcharts that represent the proportion for missing values. Between 0 and 1, the proportion is calculated for each feature against other variables present in the plot. Therefore, those with the most missing values would be higher than those with less null entries. As the plot is in a decreasing order, we see that both SATWR25 and SATWR75 in this case has the most missing values compared to the other variables. This pattern is consistent with our observation of missing data in previous plot, as well as the right graph that shows Pattern for Missing Values. In this case, pink colors represent missing values and blue the non-missing ones. Another pattern to notice is that there are rows with all missing values for all variables and one with no missing values for all variables in the plot. In addition to the graph, we can observe a single variable such as WOMENONLY with less missing value. This variable may represent enrolled students with female gener who take a standardized test. This makes sense because gender is one of the easiest profile questions to fill and thus almost all students would have filled it out. The one line with missing data pattern is just a null entry across all features that can be removed from the dataset. One possible bias might be that sometimes the profiles including gender entries are completely filled out but not other identifiers such as their occupation or most recent degree. 
